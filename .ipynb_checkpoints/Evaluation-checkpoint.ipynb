{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab033b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import SiamesePairs, SiameseTriplets, SiameseQuadruplets\n",
    "from networks import *\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pandas import DataFrameFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# plotting\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydotplus as pydot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187bdb1",
   "metadata": {},
   "source": [
    "## PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58353615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_roc(predictions, labels):\n",
    "    \"\"\"Compute ROC accuracyand threshold.\n",
    "\n",
    "    Also, plot FAR-FRR curves and P-R curves for input data.\n",
    "    \n",
    "    Args:\n",
    "        predictions -- np.array : array of predictions.\n",
    "        labels -- np.array : true labels (0 or 1).\n",
    "        plot_far_frr -- bool : plots curves of True.\n",
    "    \n",
    "    Returns:\n",
    "        max_acc -- float : maximum accuracy of model.\n",
    "        best_thresh --float : best threshold for the model.\n",
    "    \"\"\"\n",
    "    dmax = np.max(predictions)\n",
    "    dmin = np.min(predictions)\n",
    "\n",
    "    nsame = np.sum(labels == 1)  #similar\n",
    "    ndiff = np.sum(labels == 0)  #different\n",
    "\n",
    "    step = 0.01\n",
    "    max_acc = 0\n",
    "    best_thresh = -1\n",
    "\n",
    "    frr_plot = []\n",
    "    far_plot = []\n",
    "    pr_plot = []\n",
    "    re_plot = []\n",
    "\n",
    "    ds = []\n",
    "    for d in np.arange(dmin, dmax+step, step):\n",
    "        idx1 = predictions.ravel() <= d  # guessed genuine\n",
    "        idx2 = predictions.ravel() > d   # guessed forged\n",
    "\n",
    "        tp = float(np.sum(labels[idx1] == 1))\n",
    "        tn = float(np.sum(labels[idx2] == 0))\n",
    "        fp = float(np.sum(labels[idx1] == 0))\n",
    "        fn = float(np.sum(labels[idx2] == 1))\n",
    "\n",
    "        tpr = float(np.sum(labels[idx1] == 1)) / nsame       \n",
    "        tnr = float(np.sum(labels[idx2] == 0)) / ndiff\n",
    "        \n",
    "        \n",
    "        acc = 0.5 * (tpr + tnr)\n",
    "        pr = tp / (tp + fp)\n",
    "        re = tp / (tp + fn)\n",
    "       \n",
    "        if (acc > max_acc):\n",
    "            max_acc, best_thresh = acc, d\n",
    "\n",
    "        far = fp / (fp + tn)\n",
    "        frr = fn / (fn + tp)\n",
    "        frr_plot.append(frr)\n",
    "        pr_plot.append(pr)\n",
    "        re_plot.append(re)\n",
    "        far_plot.append(far)\n",
    "        ds.append(d)\n",
    "\n",
    "    plot_metrics = [ds, far_plot, frr_plot, pr_plot, re_plot]\n",
    "\n",
    "    return max_acc, best_thresh, plot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30008ec0",
   "metadata": {},
   "source": [
    "## TRIPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a44df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea06ee7c",
   "metadata": {},
   "source": [
    "## QUADRUPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fdf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc27b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(data, model, training_mode):\n",
    "    print(\"\\nEvaluating model...\", end=\"\")\n",
    "\n",
    "    if training_mode == 'pairs':\n",
    "        pred = model.predict(data.pairs)\n",
    "        acc, thresh, plot_metrics = compute_accuracy_roc(pred, data.targets)\n",
    "\n",
    "        ACCURACIES.append(acc)\n",
    "        THRESHOLDS.append(thresh)\n",
    "        PLOTS.append(plot_metrics)\n",
    "\n",
    "    elif training_mode == 'triplets':\n",
    "        X = data.triplets\n",
    "        y = None\n",
    "\n",
    "    elif training_mode == 'quadruplets':\n",
    "        X = data.quadruplets\n",
    "        y = None\n",
    "\n",
    "    print(\"evaluation finished!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a90865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_plots(metrics):\n",
    "    ds = metrics[0]\n",
    "    far_plot = metrics[1]\n",
    "    frr_plot = metrics[2]\n",
    "    pr_plot = metrics[3]\n",
    "    re_plot = metrics[4]\n",
    "\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    # error rate\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(ds, far_plot, color='red')\n",
    "    ax.plot(ds, frr_plot, color='blue')\n",
    "    ax.set_title('Error rate')\n",
    "    ax.legend(['FAR', 'FRR'])\n",
    "    ax.set(xlabel = 'Thresholds', ylabel='Error rate')\n",
    "\n",
    "    # precision-recall curve\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    ax1.plot(ds, pr_plot, color='green')\n",
    "    ax1.plot(ds, re_plot, color='magenta')\n",
    "    ax1.set_title('P-R curve')\n",
    "    ax1.legend(['Precision', 'Recall'])\n",
    "    ax.set(xlabel = 'Thresholds', ylabel='Error rate')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76dd8c",
   "metadata": {},
   "source": [
    "## Everything put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS = [\n",
    "    \"weights\\\\cedar_siamese.h5\",\n",
    "    \"weights\\cedar_weights\\cedar_siamese3.h5\",\n",
    "    \"weights\\cedar_weights\\cedar_siamese3.h5\"\n",
    "]\n",
    "\n",
    "PATHS = [\n",
    "    [\n",
    "        'data\\\\pickle-files\\\\cedar_pairs1_train.pickle',\n",
    "        'data\\\\pickle-files\\\\cedar_pairs1_pairs.pickle',\n",
    "        'data\\\\pickle-files\\\\cedar_pairs1_targets.pickle'\n",
    "    ],\n",
    "    [\n",
    "        \"data\\\\pickle-files\\\\bengali_pairs1_pairs.pickle\"\n",
    "        'data\\\\pickle-files\\\\bengali_pairs1_train.pickle',\n",
    "        'data\\\\pickle-files\\\\bengali_pairs1_targets.pickle'\n",
    "    ],\n",
    "    [\n",
    "        'data\\\\pickle-files\\\\hindi_pairs1_train.pickle',\n",
    "        'data\\\\pickle-files\\\\hindi_pairs1_pairs.pickle',\n",
    "        'data\\\\pickle-files\\\\hindi_pairs1_targets.pickle'\n",
    "    ]\n",
    "]\n",
    "\n",
    "# evaluation\n",
    "ACCURACIES = []\n",
    "THRESHOLDS = []\n",
    "PLOTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'pairs'\n",
    "\n",
    "dataset = load_dataset(MODE, TO_RESET)  # loading dataset\n",
    "data_shapes(dataset)                    # seeing dataset\n",
    "model = model_setup_pairs(MODE, True)   # setting up model and training\n",
    "model.load_weights()                    # load weights\n",
    "model_evaluation(model, MODE)           # evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'triplets'\n",
    "\n",
    "dataset = load_dataset(MODE, TO_RESET)   # loading dataset\n",
    "data_shapes(dataset)                     # seeing dataset\n",
    "model = model_setup_triplet(MODE, True)  # setting up model and training\n",
    "model.load_weights()                     # load weights\n",
    "model_evaluation(model, MODE)            # evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77175f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'quadruplets'\n",
    "\n",
    "dataset = load_dataset(MODE, TO_RESET)      # loading dataset\n",
    "data_shapes(dataset)                        # seeing dataset\n",
    "model = model_setup_quadruplet(MODE, True)  # setting up model and training\n",
    "model.load_weights()                        # load weights\n",
    "model_evaluation(model, MODE)               # evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for met in PLOTS:\n",
    "    evaluation_plots(met)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
