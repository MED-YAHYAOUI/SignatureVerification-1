{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Convolutional Neural Network<br>(Triplet Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import SiameseTriplets\n",
    "from triplet_utils import *\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import wandb\n",
    "\n",
    "# plotting\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydotplus as pydot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project_path = './{0}/'.format(projectName)\n",
    "model_path = '../azurenb_temp/{0}/'.format(projectName)\n",
    "\n",
    "if not path.exists(project_path):\n",
    "    os.mkdir(project_path)\n",
    "\n",
    "if not path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./weights'):\n",
    "    os.makedirs('./weights')\n",
    "    print(\"Weights directory created\")\n",
    "else:\n",
    "    print(\"Weights directory exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shapes(data):\n",
    "    print(\"\\nNumber of classes   : \", data.train_images.shape[0])\n",
    "    print(\"Original signatures : \", len(data.train_images[0][0]))\n",
    "    print(\"Forged signatures   : \", len(data.train_images[0][1]))\n",
    "    print(\"Image shape         : \", data.train_images[0][0][0].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplets(data):\n",
    "    fig, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(8,8))\n",
    "\n",
    "    ax[0].imshow(data.triplets[0][0])\n",
    "    ax[1].imshow(data.triplets[1][0])\n",
    "    ax[2].imshow(data.triplets[2][0])\n",
    "    # subplot titles\n",
    "    ax[0].set_title('Anchor')\n",
    "    ax[1].set_title('Positive')\n",
    "    ax[2].set_title('Negative')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, weights_name):\n",
    "    print(\"\\nStarting training!\\n\")\n",
    "\n",
    "    # hyperparameters\n",
    "    EPOCHS = 100  # number of epochs\n",
    "    BS = 128  # batch size\n",
    "\n",
    "    # callbacks\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1,)]\n",
    "\n",
    "    history = model.fit(\n",
    "        pairs, targets,\n",
    "        batch_size=BS,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        validation_split=0.3,\n",
    "    )\n",
    "\n",
    "    ALL_HISTORY.append(history)\n",
    "\n",
    "    print(\"\\nSaving weight for model...\", end=\"\")\n",
    "    siamese_contrastive.save_weights('./weights/{0}.h5'.format(weights_name))\n",
    "    print(\"saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = \"../input/handwritten-signature-datasets/CEDAR/CEDAR\"  # path to dataset (kaggle)\n",
    "# SAVE_PATH = \"./\"                                                   # path to save pickle files (kaggle)\n",
    "\n",
    "DATA_PATH = \"data\\\\CEDAR\"             # path to dataset\n",
    "SAVE_PATH = \"data\\\\pickle-files\"      # path to save pickle files\n",
    "\n",
    "CLASSES = len(os.listdir(DATA_PATH))  # number of classes\n",
    "NAME = \"CEDAR\"\n",
    "\n",
    "# size of images\n",
    "SIZE = 224\n",
    "CHANNELS = 1\n",
    "INPUT_SHAPE = (SIZE, SIZE, CHANNELS)\n",
    "\n",
    "# evaluation\n",
    "ALL_HISTORY = []\n",
    "ACCURACIES = []\n",
    "THRESHOLDS = []\n",
    "PLOTS = []\n",
    "\n",
    "TO_RESET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TPU = False\n",
    "\n",
    "if IS_TPU:\n",
    "    # detect and init the TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "    # instantiate a distribution strategy\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "dataset = SiameseTriplets(name=NAME,\n",
    "                          data_path=DATA_PATH,\n",
    "                          save_path=SAVE_PATH,\n",
    "                          channels=CHANNELS,\n",
    "                          size=SIZE,\n",
    "                          reset=reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shapes(dataset)    # seeing dataset\n",
    "plot_triplets(dataset)  # plotting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "alpha1 = 1\n",
    "alpha2 = 0.5\n",
    "embeddingsize = 10\n",
    "nb_test_class = 10\n",
    "\n",
    "evaluate_every = 1000 # interval for evaluating on one-shot tasks\n",
    "n_iter = 10000        # No. of training iterations\n",
    "log_every = 50\n",
    "sample_batch_size = 16\n",
    "\n",
    "optimizer = Adam(lr = 0.00006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the embedding and network\n",
    "embedding = embedding_net(embeddingsize, INPUT_SHAPE)\n",
    "siamese_network3 = build_triplet_model(INPUT_SHAPE, embedding, margin=1)\n",
    "\n",
    "siamese_network3.compile(loss=None,optimizer=optimizer)\n",
    "\n",
    "siamese_network3.summary()\n",
    "plot_model(siamese_network3, show_shapes=True, show_layer_names=True, to_file='TRIPLETmodel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectName = \"CEDAR_triplet_loss\"\n",
    "\n",
    "# wandb config\n",
    "wandb.init(project=projectName)\n",
    "wandb.config.alpha1 = alpha1\n",
    "wandb.config.alpha2 = alpha2\n",
    "wandb.config.sample_batch_size = sample_batch_size\n",
    "wandb.config.learningrate = K.eval(optimizer.lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    microtask_start = time.time()\n",
    "    triplets = dataset.generate_triplets(16)\n",
    "    timetogetbatch = time.time()-microtask_start\n",
    "    \n",
    "    microtask_start = time.time()\n",
    "    loss = siamese_network3.train_on_batch(triplets, None)\n",
    "    timebatch3 = time.time()-microtask_start\n",
    "\n",
    "    microtask_start = time.time()\n",
    "\n",
    "    n_iteration += 1\n",
    "\n",
    "    if i % log_every == 0:\n",
    "        wandb.log({'loss3x': loss}, step=n_iteration)\n",
    "\n",
    "    if i % evaluate_every == 0:\n",
    "        elapsed_minutes = (time.time()-t_start)/60.0\n",
    "        rate = i/elapsed_minutes\n",
    "        eta = datetime.now() + timedelta(minutes=(n_iter-i)/rate)\n",
    "        eta = eta + timedelta(hours=0) #french time\n",
    "\n",
    "        print(\"[{3}] iteration {0}: {1:.1f} iter/min, Train Loss: {2} , eta : {4}\".format(\n",
    "            i, rate, loss, n_iteration, eta.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        ))\n",
    "\n",
    "        network3_train.save_weights('{1}3x-temp_weights_{0:08d}.h5'.format(n_iteration, model_path))\n",
    "\n",
    "# Final save\n",
    "network3_train.save_weights('{1}3x-temp_weights_{0:08d}.h5'.format(n_iteration, model_path))\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_roc(predictions, labels):\n",
    "    \"\"\"Compute ROC accuracyand threshold.\n",
    "\n",
    "    Also, plot FAR-FRR curves and P-R curves for input data.\n",
    "    \n",
    "    Args:\n",
    "        predictions -- np.array : array of predictions.\n",
    "        labels -- np.array : true labels (0 or 1).\n",
    "        plot_far_frr -- bool : plots curves of True.\n",
    "    \n",
    "    Returns:\n",
    "        max_acc -- float : maximum accuracy of model.\n",
    "        best_thresh --float : best threshold for the model.\n",
    "    \"\"\"\n",
    "    dmax = np.max(predictions)\n",
    "    dmin = np.min(predictions)\n",
    "\n",
    "    nsame = np.sum(labels == 1)  #similar\n",
    "    ndiff = np.sum(labels == 0)  #different\n",
    "\n",
    "    step = 0.01\n",
    "    max_acc = 0\n",
    "    best_thresh = -1\n",
    "\n",
    "    frr_plot = []\n",
    "    far_plot = []\n",
    "    pr_plot = []\n",
    "    re_plot = []\n",
    "\n",
    "    ds = []\n",
    "    for d in np.arange(dmin, dmax+step, step):\n",
    "        idx1 = predictions.ravel() <= d  # guessed genuine\n",
    "        idx2 = predictions.ravel() > d   # guessed forged\n",
    "\n",
    "        tp = float(np.sum(labels[idx1] == 1))\n",
    "        tn = float(np.sum(labels[idx2] == 0))\n",
    "        fp = float(np.sum(labels[idx1] == 0))\n",
    "        fn = float(np.sum(labels[idx2] == 1))\n",
    "\n",
    "        tpr = float(np.sum(labels[idx1] == 1)) / nsame       \n",
    "        tnr = float(np.sum(labels[idx2] == 0)) / ndiff\n",
    "        \n",
    "        \n",
    "        acc = 0.5 * (tpr + tnr)\n",
    "        pr = tp / (tp + fp)\n",
    "        re = tp / (tp + fn)\n",
    "       \n",
    "        if (acc > max_acc):\n",
    "            max_acc, best_thresh = acc, d\n",
    "\n",
    "        far = fp / (fp + tn)\n",
    "        frr = fn / (fn + tp)\n",
    "        frr_plot.append(frr)\n",
    "        pr_plot.append(pr)\n",
    "        re_plot.append(re)\n",
    "        far_plot.append(far)\n",
    "        ds.append(d)\n",
    "\n",
    "    plot_metrics = [ds, far_plot, frr_plot, pr_plot, re_plot]\n",
    "\n",
    "    return max_acc, best_thresh, plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['loss', 'val_loss']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for x in losses:\n",
    "    plt.plot(ALL_HISTORY[0].history[x])\n",
    "\n",
    "plt.title('Losses')\n",
    "plt.legend(losses)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fc04cdb9730b0f31da2f90a691cdd0fa9da7bc1222eb1adb5ee73ed7bedeaa9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
