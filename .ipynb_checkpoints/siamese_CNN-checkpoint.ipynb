{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Convolutional Neural Network\n",
    "The main model trained on the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform, he_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as keras_backend\n",
    "from keras.utils import plot_model, normalize\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import import_ipynb\n",
    "import create_dataset  # own module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_ram(a):\n",
    "    \"\"\"Function to free ram by deleting variables.\n",
    "\n",
    "    Args:\n",
    "        a : variable to be deleted.\n",
    "    \"\"\"\n",
    "    a = []\n",
    "    del a[:]\n",
    "    del a\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_path = \"data\\\\CEDAR\"\n",
    "bengali_path = \"data\\\\BHSig260-Bengali\"\n",
    "hindi_path = \"data\\\\BHSig260-Hindi\"\n",
    "PATHS = [english_path, bengali_path, hindi_path]\n",
    "\n",
    "english_classes = 55\n",
    "bengali_classes = 100\n",
    "hindi_classes = 160\n",
    "CLASSES = [english_classes, bengali_classes, hindi_classes]\n",
    "\n",
    "SIZE = 224\n",
    "input_shape = (224, 224, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = []\n",
    "ORIGINALS = []\n",
    "\n",
    "for i, j in zip(PATHS, CLASSES):\n",
    "    data_train, data_test, x_train_origin, y_train_origin, x_test_origin, y_test_origin = create_dataset.build_dataset(i, j)\n",
    "\n",
    "    DATASETS.append([data_train, data_test])\n",
    "    ORIGINALS.append([x_train_origin, y_train_origin, x_test_origin, y_test_origin])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in DATASETS:\n",
    "    print(\"\\n\", DATASETS.index(data))\n",
    "    print(\"Checking shapes for class 0 (train) : \", data[0][0].shape)\n",
    "    print(\"Checking shapes for class 0 (test) : \", data[1][0].shape)\n",
    "\n",
    "    draw_pics(data[0][0], nb=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [data_train, data_test, x_train_origin, y_train_origin, x_test_origin, y_test_origin]\n",
    "for i in x:\n",
    "    free_ram(i)\n",
    "free_ram(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_shape, embeddingsize):\n",
    "    \"\"\"Define the neural network to learn image similarity.\n",
    "\n",
    "    Args:\n",
    "        input_shape -- tuple : shape of input images.\n",
    "        embeddingsize -- int : vector size used to encode our picture.\n",
    "    \"\"\"\n",
    "    # Convolutional Neural Network\n",
    "    network = Sequential()\n",
    "    network.add(Conv2D(128, (7, 7), activation='relu',\n",
    "                       input_shape=input_shape,\n",
    "                       kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "\n",
    "    network.add(Conv2D(128, (3, 3), activation='relu',\n",
    "                       kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "\n",
    "    network.add(Conv2D(256, (3, 3), activation='relu',\n",
    "                       kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=l2(2e-4)))\n",
    "    network.add(Flatten())\n",
    "\n",
    "    network.add(Dense(4096, activation='relu',\n",
    "                      kernel_regularizer=l2(1e-3),\n",
    "                      kernel_initializer='he_uniform'))\n",
    "\n",
    "    network.add(Dense(embeddingsize, activation=None,\n",
    "                      kernel_regularizer=l2(1e-3),\n",
    "                      kernel_initializer='he_uniform'))\n",
    "\n",
    "    # Force the encoding to live on the d-dimentional hypershpere\n",
    "    network.add(Lambda(lambda x: keras_backend.l2_normalize(x, axis=-1)))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    \"\"\"Class to calculate triplet loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        \"\"\"Constructor for TripletLossLayer class.\"\"\"\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def triplet_loss(self, inputs):\n",
    "        \"\"\"Module to calculate triplet loss.\"\"\"\n",
    "        anchor, positive, negative = inputs\n",
    "        p_dist = keras_backend.sum(keras_backend.square(anchor-positive), axis=-1)\n",
    "        n_dist = keras_backend.sum(keras_backend.square(anchor-negative), axis=-1)\n",
    "\n",
    "        return keras_backend.sum(keras_backend.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Module to return loss.\"\"\"\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, network, margin=0.2):\n",
    "    \"\"\"Define the Keras Model for training.\n",
    "    \n",
    "    Args:\n",
    "        input_shape -- tuple : shape of input images.\n",
    "        network -- CNN : Neural network to train outputing embeddings.\n",
    "        margin -- float : minimal distance between Anchor-Positive and\n",
    "                          Anchor-Negative for the lossfunction (alpha).\n",
    "    \"\"\"\n",
    "    # Define the tensors for the three input images\n",
    "    anchor_input = Input(input_shape, name=\"anchor_input\")\n",
    "    positive_input = Input(input_shape, name=\"positive_input\")\n",
    "    negative_input = Input(input_shape, name=\"negative_input\")\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the three images\n",
    "    encoded_a = network(anchor_input)\n",
    "    encoded_p = network(positive_input)\n",
    "    encoded_n = network(negative_input)\n",
    "\n",
    "    # TripletLoss Layer\n",
    "    loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([\n",
    "        encoded_a, encoded_p, encoded_n])\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    network_train = Model(inputs=[anchor_input, positive_input, negative_input], outputs=loss_layer)\n",
    "\n",
    "    # return the model\n",
    "    return network_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_random(batch_size, s=\"train\"):\n",
    "    \"\"\"Create batch of APN triplets with a complete random strategy.\n",
    "\n",
    "    Args:\n",
    "        batch_size -- int : the batch size.\n",
    "\n",
    "    Returns:\n",
    "        triplets -- list : list containing 3 tensors Anchor, Positive,\n",
    "                           Negative of shape (batch_size, w, h, c).\n",
    "    \"\"\"\n",
    "    if s == 'train':\n",
    "        X = dataset_train\n",
    "    else:\n",
    "        X = dataset_test\n",
    "\n",
    "    m, w, h, c = X[0].shape\n",
    "\n",
    "    # initialize result\n",
    "    triplets = [np.zeros((batch_size, h, w, c)) for i in range(3)]\n",
    "    \n",
    "    '''\n",
    "    if '-G-' or 'original' in img_path:\n",
    "        labels.append(np.array(1))\n",
    "    else:\n",
    "        labels.append(np.array(0))\n",
    "    '''\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Pick one random class for anchor\n",
    "        anchor_class = np.random.randint(0, nb_classes)\n",
    "        nb_sample_available_for_class_AP = X[anchor_class].shape[0]\n",
    "\n",
    "        # Pick two different random pics for this class => A and P\n",
    "        [idx_A, idx_P] = np.random.choice(nb_sample_available_for_class_AP, size=2, replace=False)\n",
    "\n",
    "        # Pick another class for N, different from anchor_class\n",
    "        negative_class = (anchor_class + np.random.randint(1, nb_classes)) % nb_classes\n",
    "        nb_sample_available_for_class_N = X[negative_class].shape[0]\n",
    "\n",
    "        # Pick a random pic for this negative class => N\n",
    "        idx_N = np.random.randint(0, nb_sample_available_for_class_N)\n",
    "\n",
    "        triplets[0][i, :, :, :] = X[anchor_class][idx_A, :, :, :]\n",
    "        triplets[1][i, :, :, :] = X[anchor_class][idx_P, :, :, :]\n",
    "        triplets[2][i, :, :, :] = X[negative_class][idx_N, :, :, :]\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(a, b):\n",
    "    return np.sum(np.square(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_hard(draw_batch_size, hard_batchs_size, norm_batchs_size, network, s=\"train\"):\n",
    "    \"\"\"Create batch of APN \"hard\" triplets.\n",
    "\n",
    "    Args:\n",
    "        draw_batch_size -- int : number of initial randomly taken samples.\n",
    "        hard_batchs_size -- int : select the number of hardest samples to keep.\n",
    "        norm_batchs_size -- int : number of random samples to add.\n",
    "\n",
    "    Returns:\n",
    "        triplets -- list : containing 3 tensors Anchor, Positive, Negative of\n",
    "                           shape (hard_batchs_size+norm_batchs_size, w, h, c).\n",
    "    \"\"\"\n",
    "    if s == 'train':\n",
    "        X = dataset_train\n",
    "    else:\n",
    "        X = dataset_test\n",
    "\n",
    "    m, w, h, c = X[0].shape\n",
    "\n",
    "    # Step 1 : pick a random batch to study\n",
    "    studybatch = get_batch_random(draw_batch_size, s)\n",
    "\n",
    "    # Step 2 : compute the loss with current network : d(A,P)-d(A,N).\n",
    "    # The alpha parameter here is omited here since we want only to order them\n",
    "    studybatchloss = np.zeros((draw_batch_size))\n",
    "\n",
    "    # Compute embeddings for anchors, positive and negatives\n",
    "    A = network.predict(studybatch[0])\n",
    "    P = network.predict(studybatch[1])\n",
    "    N = network.predict(studybatch[2])\n",
    "\n",
    "    # Compute d(A,P)-d(A,N)\n",
    "    studybatchloss = np.sum(np.square(A-P), axis=1) - \\\n",
    "        np.sum(np.square(A-N), axis=1)\n",
    "\n",
    "    # Sort by distance (high distance first) and take the\n",
    "    selection = np.argsort(studybatchloss)[::-1][:hard_batchs_size]\n",
    "\n",
    "    # Draw other random samples from the batch\n",
    "    selection2 = np.random.choice(np.delete(np.arange(draw_batch_size), selection),\n",
    "                                            norm_batchs_size,\n",
    "                                            replace=False)\n",
    "\n",
    "    selection = np.append(selection, selection2)\n",
    "\n",
    "    triplets = [studybatch[0][selection, :, :, :],\n",
    "                studybatch[1][selection, :, :, :],\n",
    "                studybatch[2][selection, :, :, :]]\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_triplets(tripletbatch, nbmax=None):\n",
    "    \"\"\"Display the three images for each triplets in the batch.\n",
    "\n",
    "    Args:\n",
    "        tripletbatch -- : image batch of triplets.\n",
    "        nbmax -- int/None : number of batches to be displayed,\n",
    "                            if set to None will show all batches.\n",
    "    \"\"\"\n",
    "    labels = [\"Anchor\", \"Positive\", \"Negative\"]\n",
    "\n",
    "    if (nbmax == None):\n",
    "        nbrows = tripletbatch[0].shape[0]\n",
    "    else:\n",
    "        nbrows = min(nbmax, tripletbatch[0].shape[0])\n",
    "\n",
    "    for row in range(nbrows):\n",
    "        fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "        for i in range(3):\n",
    "            subplot = fig.add_subplot(1, 3, i+1)\n",
    "            axis(\"off\")\n",
    "            plt.imshow(tripletbatch[i][row, :, :, 0], vmin=0, vmax=1, cmap='Greys')\n",
    "            subplot.title.set_text(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(network, X, Y):\n",
    "    \"\"\"Compute probs.\n",
    "\n",
    "    Args:\n",
    "        network : current NN to compute embeddings.\n",
    "        X : tensor of shape (m,w,h,1) containing pics to evaluate.\n",
    "        Y : tensor of shape (m,) containing true class.\n",
    "\n",
    "    Returns:\n",
    "        probs : array of shape (m, m) containing distances.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    nbevaluation = int(m*(m-1)/2)\n",
    "    probs = np.zeros((nbevaluation))\n",
    "    y = np.zeros((nbevaluation))\n",
    "\n",
    "    # Compute all embeddings for all pics with current network\n",
    "    embeddings = network.predict(X)\n",
    "\n",
    "    size_embedding = embeddings.shape[1]\n",
    "\n",
    "    # For each pics of our dataset\n",
    "    k = 0\n",
    "    for i in range(m):\n",
    "        # Against all other images\n",
    "        for j in range(i+1, m):\n",
    "            # Compute the probability of being the right decision : it should be 1 for right class, 0 for all other classes\n",
    "            probs[k] = -compute_dist(embeddings[i, :], embeddings[j, :])\n",
    "            if (Y[i] == Y[j]):\n",
    "                y[k] = 1\n",
    "                #print(\"{3}:{0} vs {1} : {2}\\tSAME\".format(i,j,probs[k],k))\n",
    "            else:\n",
    "                y[k] = 0\n",
    "                #print(\"{3}:{0} vs {1} : \\t\\t\\t{2}\\tDIFF\".format(i,j,probs[k],k))\n",
    "            k += 1\n",
    "    return probs, y\n",
    "#probs,yprobs = compute_probs(network,x_test_origin[:10,:,:,:],y_test_origin[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(probs, yprobs):\n",
    "    \"\"\" Computer metrics.\n",
    "\n",
    "    Args:\n",
    "        probs -- :\n",
    "        yprobs -- :\n",
    "\n",
    "    Returns:\n",
    "        fpr : Increasing false positive rates such that element i is the false positive rate \n",
    "              of predictions with score >= thresholds[i].\n",
    "        tpr : Increasing true positive rates such that element i is the true positive rate \n",
    "              of predictions with score >= thresholds[i].\n",
    "        thresholds : Decreasing thresholds on the decision function used to compute fpr and tpr.\n",
    "                     thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1\n",
    "        auc : Area Under the ROC Curve metric.\n",
    "    \"\"\"\n",
    "    # calculate AUC\n",
    "    auc = roc_auc_score(yprobs, probs)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(yprobs, probs)\n",
    "\n",
    "    return fpr, tpr, thresholds, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interdist(network):\n",
    "    \"\"\"Computes sum of distances between all classes embeddings on our reference test image.\n",
    "\n",
    "    Distances are calculated as such:\n",
    "    d(0,1) + d(0,2) + ... + d(0,9) + d(1,2) + d(1,3) + ... d(8,9)\n",
    "    A good model should have a large distance between all theses embeddings.\n",
    "\n",
    "    Args:\n",
    "        network -- CNN : current CNN.\n",
    "\n",
    "    Returns:\n",
    "        res -- np.array : array of shape (nb_classes,nb_classes).\n",
    "    \"\"\"\n",
    "    res = np.zeros((nb_classes, nb_classes))\n",
    "\n",
    "    ref_images = np.zeros((nb_classes, img_rows, img_cols, 1))\n",
    "\n",
    "    # generates embeddings for reference images\n",
    "    for i in range(nb_classes):\n",
    "        ref_images[i, :, :, :] = dataset_test[i][0, :, :, :]\n",
    "    ref_embeddings = network.predict(ref_images)\n",
    "\n",
    "    for i in range(nb_classes):\n",
    "        for j in range(nb_classes):\n",
    "            res[i, j] = compute_dist(ref_embeddings[i], ref_embeddings[j])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_interdist(network, n_iteration):\n",
    "    \"\"\"Plots embeddings distance from each other after n iterations.\n",
    "\n",
    "    Args:\n",
    "        network -- CNN : current CNN.\n",
    "        n_iteraction -- int : current iteration.\n",
    "    \"\"\"\n",
    "    interdist = compute_interdist(network)\n",
    "\n",
    "    data = []\n",
    "    for i in range(nb_classes):\n",
    "        data.append(np.delete(interdist[i, :], [i]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Evaluating embeddings distance from each other after {0} iterations\".format(n_iteration))\n",
    "    ax.set_ylim([0, 3])\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Distance')\n",
    "    ax.boxplot(data, showfliers=False, showbox=True)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks(locs, np.arange(nb_classes))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    \"\"\"Find nearest value.\n",
    "\n",
    "    Args:\n",
    "        array -- :\n",
    "        value -- :\n",
    "    \n",
    "    Returns:\n",
    "        array[idx-1] -- np.array : nearest value from array.\n",
    "        idx -- int : index.\n",
    "    \"\"\"\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return array[idx-1], idx-1\n",
    "    else:\n",
    "        return array[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc(fpr, tpr, thresholds):\n",
    "    \"\"\"Draw ROC under curve.\n",
    "\n",
    "    Args:\n",
    "        fpr -- :\n",
    "        tpr -- :\n",
    "        thresholds -- :\n",
    "    \"\"\"\n",
    "    # Find threshold\n",
    "    targetfpr = 1e-3\n",
    "    _, idx = find_nearest(fpr, targetfpr)\n",
    "    threshold = thresholds[idx]\n",
    "    recall = tpr[idx]\n",
    "\n",
    "    # Plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # Plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.title('AUC: {0:.3f}\\nSensitivity : {2:.1%} @FPR={1:.0e}\\nThreshold={3})'.format(\n",
    "        auc, targetfpr, recall, abs(threshold)))\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_test_image(network, images, refidx=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        network -- CNN :\n",
    "        images -- :\n",
    "        refidx -- :\n",
    "\n",
    "    Returns:\n",
    "        scores : resultat des scores de similaritÃ©s avec les images de base => (N)\n",
    "    \"\"\"\n",
    "    N = 4\n",
    "    _, w, h, c = dataset_test[0].shape\n",
    "    nbimages = images.shape[0]\n",
    "\n",
    "    # generates embedings for given images\n",
    "    image_embedings = network.predict(images)\n",
    "\n",
    "    # generates embedings for reference images\n",
    "    ref_images = np.zeros((nb_classes, w, h, c))\n",
    "    for i in range(nb_classes):\n",
    "        ref_images[i, :, :, :] = dataset_test[i][refidx, :, :, :]\n",
    "    ref_embedings = network.predict(ref_images)\n",
    "\n",
    "    for i in range(nbimages):\n",
    "        # Prepare the figure\n",
    "        fig = plt.figure(figsize=(16, 2))\n",
    "        subplot = fig.add_subplot(1, nb_classes+1, 1)\n",
    "        axis(\"off\")\n",
    "        plotidx = 2\n",
    "\n",
    "        # Draw this image\n",
    "        plt.imshow(images[i, :, :, 0], vmin=0, vmax=1, cmap='Greys')\n",
    "        subplot.title.set_text(\"Test image\")\n",
    "\n",
    "        for ref in range(nb_classes):\n",
    "            # Compute distance between this images and references\n",
    "            dist = compute_dist(image_embedings[i, :], ref_embedings[ref, :])\n",
    "            # Draw\n",
    "            subplot = fig.add_subplot(1, nb_classes+1, plotidx)\n",
    "            axis(\"off\")\n",
    "            plt.imshow(ref_images[ref, :, :, 0], vmin=0, vmax=1, cmap='Greys')\n",
    "            subplot.title.set_text((\"Class {0}\\n{1:.3e}\".format(ref, dist)))\n",
    "            plotidx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 1000  # Interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 80000  # No. of training iterations\n",
    "n_val = 250  # How many one-shot tasks to validate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing on an untrained network\n",
    "# probs, yprob = compute_probs(network, x_test_origin[:500, :, :, :], y_test_origin[:500])\n",
    "# fpr, tpr, thresholds, auc = compute_metrics(probs, yprob)\n",
    "# draw_roc(fpr, tpr, thresholds)\n",
    "# draw_interdist(network, n_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     draw_test_image(network, np.expand_dims(dataset_train[i][0, :, :, :], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplets = get_batch_random(2)\n",
    "# print(\"Checking batch width, should be 3 : \", len(triplets))\n",
    "# print(\"Shapes in the batch A:{0} P:{1} N:{2}\".format(triplets[0].shape, triplets[1].shape, triplets[2].shape))\n",
    "# draw_triplets(triplets)\n",
    "\n",
    "# hardtriplets = get_batch_hard(50, 1, 1, network)\n",
    "# print(\"Checking hard batch width, should be 3 : \", len(hardtriplets))\n",
    "# print(\"Shapes in the hardbatch A:{0} P:{1} N:{2}\".format(hardtriplets[0].shape, hardtriplets[1].shape, hardtriplets[2].shape))\n",
    "# draw_triplets(hardtriplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_train = build_model(input_shape, build_network(input_shape, embeddingsize=10))\n",
    "\n",
    "optimizer = Adam(lr=0.00006)\n",
    "network_train.compile(loss=None, optimizer=optimizer)\n",
    "network_train.summary()\n",
    "\n",
    "plot_model(network_train,\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           to_file='Model Plot.png')\n",
    "\n",
    "print(network_train.metrics_names)\n",
    "n_iteration = 0\n",
    "network_train.load_weights('mnist-160k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "dummy_target = [np.zeros((batch_size, 15)) for i in range(3)]\n",
    "\n",
    "for i in range(1, n_iter+1):\n",
    "    triplets = get_batch_hard(200, 16, 16, network)\n",
    "    loss = network_train.train_on_batch(triplets, None)\n",
    "    n_iteration += 1\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0, loss, n_iteration))\n",
    "        probs, yprob = compute_probs(network, x_test_origin[:n_val, :, :, :], y_test_origin[:n_val])\n",
    "        #fpr, tpr, thresholds,auc = compute_metrics(probs,yprob)\n",
    "        #draw_roc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation\n",
    "probs, yprob = compute_probs(network, x_test_origin, y_test_origin)\n",
    "fpr, tpr, thresholds, auc = compute_metrics(probs, yprob)\n",
    "draw_roc(fpr, tpr, thresholds)\n",
    "draw_interdist(network, n_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    draw_test_image(network, np.expand_dims(dataset_train[i][0, :, :, :], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fc04cdb9730b0f31da2f90a691cdd0fa9da7bc1222eb1adb5ee73ed7bedeaa9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
